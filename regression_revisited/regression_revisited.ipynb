{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:30px; text-align:center\"><b>Regression Revisited</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">(C) <a href=\"https://github.com/niknow\">Nikolai Nowaczyk</a>, <a href=\"https://github.com/Lapsilago\">Jörg Kienitz</a> 2019-2021</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.lines import Line2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least squares linear regression is one of the most fundamental techniques in supervised machine learning. In this notebook, we revisit the core mathematical concepts and illustrate how to conveniently use them in Python.\n",
    "\n",
    "In theory, assume we are given a function $f:\\mathbb{R}^k \\to \\mathbb{R}$ and a vector of inputs $x=(x_1, \\ldots, x_N)$, $x_i \\in \\mathbb{R}^k$, then it is straightforward to compute the corresponding vector of ouputs $y=(y_1, \\ldots, y_N)$, $y_i := f(x_i) \\in \\mathbb{R}$. \n",
    "\n",
    "In practice, the true function $f$ is often unkown. The only information we know about a problem is the vector of inputs $x$ and the vector of outputs $y$ and we would like to *learn* the function $f$ mapping $x$ to $y$. But what function $f$ ist the best fit for the given data? As the space of functions is overcountably infinite, but the data is finite, this creates the obvious theoretical problem that the answer cannot be unique. This problem is circumvented in practice by\n",
    "* replacing the space of all functions by a smaller subspace $\\mathscr{H}$ of admissable functions, e.g. the space of polynomials up to a maximum degree\n",
    "* chosing a metric $J$ to evaluate, which element of $\\mathscr{H}$ is a best fit for the data, e.g. the squared distance\n",
    "* optimizing this process by pre-processing the input data into a *design matrix* $X$, e.g. by extracting (higher order) features or by applying scaling/normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Framework\n",
    "The mathematical framework for performing such regressions is the following.\n",
    "\n",
    "**Definition (least squares linear regression)**: Let $X \\in \\mathbb{R}^{N \\times m}$ be a *design matrix* and $y \\in \\mathbb{R}^N$ be a *response*. We identify $\\mathscr{H} = \\mathbb{R}^m$ with the space of linear polynomials in $m$ variables, i.e. for every $\\theta \\in \\mathbb{R}^m$, we define\n",
    "\\begin{align*}\n",
    "h_{\\theta}: \\mathbb{R}^m \\to \\mathbb{R}, && x \\mapsto h_{\\theta}(x) := \\sum_{j=1}^m{x_j \\theta_j}.\n",
    "\\end{align*}\n",
    "We define the *least squares function* \n",
    "\\begin{align*}\n",
    "    J:\\mathbb{R}^N \\times \\mathbb{R}^N, && (y,y') \\mapsto J(y,y') := \\frac{1}{2N} \\| y - y' \\|^2_2 = \\frac{1}{2N} \\sum_{i=1}^{N}{(y_i - y_i')^2}\n",
    "\\end{align*}\n",
    "Finally, the function $h_{\\beta}$ satisfying\n",
    "\\begin{align*}\n",
    "    J(\\beta) := J(h_{\\beta}(X),y) = \\min_{\\theta \\in \\mathbb{R}^m}{J(h_{\\theta}(X),y)}\n",
    "\\end{align*}\n",
    "is called the *least squares linear regression of $X$ against $y$*. In the expression $h_{\\theta}(X)$ we employ the convention that $h_{\\theta}$ is applied to every row of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimization problem can be solved as follows:\n",
    "\n",
    "**Theorem (normal equations)**: Let $X \\in \\mathbb{R}^{N \\times m}$ be a design matrix and $y \\in \\mathbb{R}^N$ be a response as above. Then the coefficients $\\beta$ of the least squares linear regression of $X$ against $y$ satisfy the *normal equation*\n",
    "\\begin{align*}\n",
    "    X^T X \\beta = X^T y\n",
    "\\end{align*}\n",
    "\n",
    "This equation can be proven by using the fact that at a minimum $\\beta$, the differential of the function\n",
    "\\begin{align*}\n",
    "    \\theta \\mapsto J(h_\\theta(X),y) = \\frac{1}{2N} \\sum_{i=1}^{N}{\\Big{(}\\sum_{j=1}^m{\\theta_j x_{ij}} - y_i\\Big{)}^2}\n",
    "\\end{align*}\n",
    "has to vanish.\n",
    "\n",
    "Numerically, the normal equations can be solved robustly by applying a [QR-decomposition](https://en.wikipedia.org/wiki/QR_decomposition) $X=QR$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the input data does often not come in the form of the design matrix directly and the matrix has to be assembled first. We discuss this for univariate and multivariate polynomial regression.\n",
    "\n",
    "**Definition (polynomial regression)**: Let $x=(x_1, \\ldots, x_N) \\in \\mathbb{R}^N$, and $y \\in \\mathbb{R}^{N}$ and $d \\in \\mathbb{N}$. The linear regression with the design matrix $X \\in \\mathbb{R}^{N \\times (d + 1)}$, defined by\n",
    "\\begin{align*}\n",
    "    X_{i,j} := x_i^j, \\quad i=1, \\ldots, N, \\quad j=0, \\ldots, d,\n",
    "\\end{align*}\n",
    "is called *polynomial regression of degree $d$ (with intercept)*.\n",
    "\n",
    "The matrix $X$ looks like \n",
    "\\begin{align*}\n",
    "X = \\begin{pmatrix}\n",
    "        1 & x_1 & x_1^2 & \\ldots & x_1^d \\\\\n",
    "        1 & x_2 & x_2^2 & \\ldots & x_2^d \\\\\n",
    "        \\ldots \\\\\n",
    "        1 & x_N & x_N^2 & \\ldots & x_N^d \\\\\n",
    "    \\end{pmatrix} \\in \\mathbb{R}^{N \\times (d+1)}\n",
    "\\end{align*}\n",
    "and its columns are also called *feature vectors*. The first column is called *intercept* and not always used (notice that $x^0 := 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression can also be used when the $x_i \\in \\mathbb{R}$ are not scalars, but vectors $x_i \\in \\mathbb{R}^k$ instead. \n",
    "\n",
    "**Definition (multivariate polynomial regression)**: Let $x=(x_1, \\ldots, x_N) \\in \\mathbb{R}^{k \\times N}$, $y \\in \\mathbb{R}^{N}$ and $d=(d_1, \\ldots, d_k) \\in \\mathbb{N}^k$. The linear regression with the design matrix $X \\in \\mathbb{R}^{N \\times (|d| + 1)}$, defined by\n",
    "\n",
    "\\begin{align*}\n",
    "X = \\begin{pmatrix}\n",
    "        1 & x_{11} & x_{11}^2 & \\ldots & x_{11}^{d_1} & x_{12} & x_{12}^2 & \\ldots x_{12}^{d_2} & \\ldots & x_{1k}^{d_k}  \\\\\n",
    "        1 & x_{21} & x_{21}^2 & \\ldots & x_{21}^{d_1} & x_{22} & x_{22}^2 & \\ldots x_{22}^{d_2} & \\ldots & x_{2k}^{d_k} \\\\\n",
    "        \\ldots \\\\\n",
    "        1 & x_{N1} & x_{N1}^2 & \\ldots & x_{N1}^{d_1} & x_{N2} & x_{N2}^2 & \\ldots x_{N2}^{d_2} & \\ldots & x_{Nk}^{d_k} \\\\\n",
    "    \\end{pmatrix} \\in \\mathbb{R}^{N \\times (|d|+1)}\n",
    "\\end{align*}\n",
    "\n",
    "is called *multivariate polynomial regression with degrees $d$ (and intercept)*.\n",
    "\n",
    "Chosing the various degrees $d_j$ is a non-trivial problem often discussed in connection with regularization.\n",
    "\n",
    "Notice that even multivariate polynomial regression is still an instance of linear regression. That is because the *linear* refers to the coefficients $\\beta$ in the corresponding function\n",
    "\\begin{align*}\n",
    "    h_{\\beta}: \\mathbb{R}^k \\to \\mathbb{R}, && x \\mapsto \\beta_0 + \\sum_{j=1}^{k}{\\sum_{\\nu=1}^{d_k}\\beta_{j,\\nu} x_j^{\\nu}}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Univariate Polynomial Regression\n",
    "\n",
    "We discuss some examples of linear regression using the framework in `scikit-learn`. All examples use synthetic data for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "    \n",
    "def format_axis(ax):\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.grid()\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Through Origin\n",
    "This is one of the easiest examples of linear regression: A line fit through the origin. Notice that we set  `fit_intercept=False` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda3\\envs\\heroku\\lib\\site-packages\\matplotlib\\__init__.py:942: MatplotlibDeprecationWarning: nbagg.transparent is deprecated and ignored. Use figure.facecolor instead.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53745d3352b94d12a387dbcfc66cf79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-1,1,100)\n",
    "y = x + np.random.normal(0, 0.1, 100)\n",
    "reg = LinearRegression(fit_intercept=False).fit(x[:,np.newaxis],y)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, label='train', color='tab:blue')\n",
    "ax.plot(x, reg.predict(x.reshape(-1, 1)), color='r', linewidth=3, label='predict')\n",
    "format_axis(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine Line\n",
    "This example fits an affine line, i.e. a line, which does not go through the origin. As we can see, this doesn't work very well, when we set `fit_intercept=False`. It does work as well the line through the origin above, if we set `fit_intercept=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda3\\envs\\heroku\\lib\\site-packages\\matplotlib\\__init__.py:942: MatplotlibDeprecationWarning: nbagg.transparent is deprecated and ignored. Use figure.facecolor instead.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257160b445824c7bafe5f2281dbc32f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-1,1,100)\n",
    "y = x + 1 + np.random.normal(0, 0.1, 100)\n",
    "reg = LinearRegression(fit_intercept=False).fit(x[:,np.newaxis],y)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y, label='train')\n",
    "ax.plot(x,reg.predict(x.reshape(-1, 1)), color='r', linewidth=3, label='predict')\n",
    "format_axis(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parabola\n",
    "Now we consider data with a parabolic structure. As we can see, fitting a line through that data doesn't capture that structure very well (even with intercept), but fitting a parabola does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda3\\envs\\heroku\\lib\\site-packages\\matplotlib\\__init__.py:942: MatplotlibDeprecationWarning: nbagg.transparent is deprecated and ignored. Use figure.facecolor instead.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768a60e2202c4f998a732dec29f2e305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-1,1,100)\n",
    "y = x**2 + np.random.normal(0, 0.1, 100)\n",
    "reg = LinearRegression(fit_intercept=True).fit(x[:,np.newaxis],y)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y, label='train')\n",
    "ax.plot(x,reg.predict(x.reshape(-1, 1)), color='r', linewidth=3, label='predict linear')\n",
    "\n",
    "reg_parab = LinearRegression(fit_intercept=True).fit(np.array([x, x**2]).T,y)\n",
    "ax.plot(x,reg_parab.predict(np.array([x, x**2]).T), color='tab:green', linewidth=3, label='predict quadratic')\n",
    "format_axis(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial of order 100\n",
    "While the parabola from the previous example fits the data quite well, one could ask on whether or not one can do any better. After all, not all points of the training set lie exactly on the parabola. Here, we chose a polynomial of the same degree as the number of samples in the training set. As we can see from the plot below, this causes some rather unpleasant surprises: \n",
    "\n",
    "* Even though mathematically, the resulting regression polynomial should interpolate all the data points exactly, it does not do that. This is because polynomial regressions using double precision tend to fail when the degree becomes bigger than 20 due to numerical instabilities.\n",
    "* When we evalute the resulting regression polynomial at points, which are not on the training set, but in between (`x_grid` in the below code), then the values of that polynomial completely explode. This is a known problem with regression and an instance of a phenomenon called *overfitting*. \n",
    "\n",
    "$\\mathbf{\\Longrightarrow}$ **Chosing the degree of the regression polynomial as the number of training samples is not a good idea.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda3\\envs\\heroku\\lib\\site-packages\\matplotlib\\__init__.py:942: MatplotlibDeprecationWarning: nbagg.transparent is deprecated and ignored. Use figure.facecolor instead.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9427a672e00a43868b1237de21eadac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_samples = 100\n",
    "poly_d = num_samples - 1\n",
    "x = np.linspace(-1,1,num_samples)\n",
    "x_grid = np.linspace(-1,1,1000)\n",
    "y = x**2 + np.random.normal(0, 0.1, num_samples)\n",
    "X = np.array([x**d for d in range(1,poly_d+1)]).T\n",
    "X_grid = np.array([x_grid**d for d in range(1,poly_d+1)]).T\n",
    "\n",
    "reg = LinearRegression(fit_intercept=True).fit(X,y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y, label='train')\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,2])\n",
    "ax.plot(x_grid, reg.predict(X_grid), color='r', linewidth=3, label='predict')\n",
    "format_axis(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias/Variance Diagnostics for Linear Regression: Finding optimal degree\n",
    "\n",
    "In the examples above, chosing the ''right'' degree for the polynomial in the regression was fairly straightforward. When the problem is more complex, has higher dimension (and of course if the data is not synthetic), this can become more tricky. A key tool to address this is the bias/variance diagnostic, which besides the training set also relies on a cross validation set. \n",
    "\n",
    "** Definition (bias/variance)**: Let $(X_{\\operatorname{train}},y_{\\operatorname{train}})$ be a design matrix and response for a linear regression with solution $\\beta$, called *training set*, and $(X_{\\operatorname{cv}},y_{\\operatorname{cv}})$ be a data set, called *cross validation set*, such that $X_{\\operatorname{train}}$ and $X_{\\operatorname{cv}}$ have the same number of columns. Then the quantities\n",
    "\\begin{align*}\n",
    "\\text{bias} := J(h_{\\beta}(X_{\\operatorname{train}}, y_{\\operatorname{train}})), &&\n",
    "\\text{variance} := J(h_{\\beta}(X_{\\operatorname{cv}}, y_{\\operatorname{cv}}))\n",
    "\\end{align*}\n",
    "are called the *bias* and the *variance* of the regression with respect to that cross-validation set.\n",
    "\n",
    "** Remark **: \n",
    "*  By definition of the least squares regression, the bias is the size of the least squares. Thus, the bias is a metric of the training error. A low bias indicates that the regression polynomial provides a good fit on the training data and avoids *underfitting*.\n",
    "* The bias is a metric on how the polynomial fitted on the training set generalizes to points not in the training set. A low variance avoids *overfitting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "We illustrate the above concepts via an example that is again given by a synthetic data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda3\\envs\\heroku\\lib\\site-packages\\matplotlib\\__init__.py:942: MatplotlibDeprecationWarning: nbagg.transparent is deprecated and ignored. Use figure.facecolor instead.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501dd1eca34748c69898cc28d96b5418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create synthetic data set\n",
    "np.random.seed(2)\n",
    "num_samples = 100\n",
    "x_train = np.linspace(-1,1,num_samples)\n",
    "x_cv = np.linspace(-1,1,num_samples)+1/(2*num_samples)\n",
    "y_train = 5 + 6*x**2+7*x**3 + np.random.normal(0, 0.1, num_samples)\n",
    "y_cv = 5 + 6*x_cv**2+7*x_cv**3 + np.random.normal(0, 0.1, num_samples)\n",
    "\n",
    "# plot data set\n",
    "fig, ax = plt.subplots()\n",
    "dot_size = 10\n",
    "ax.scatter(x_train, y_train, s=dot_size, label='train')\n",
    "ax.scatter(x_cv,y_cv, s=dot_size, alpha=0.5, label='cv')\n",
    "format_axis(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though one might be able to eyeball the ''right'' degree of the polynomial from this plot already, we pretend we cannot and use the bias/variance diagnostic to systematically try out an (unreasonably large) number of degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematically trying out degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda3\\envs\\heroku\\lib\\site-packages\\matplotlib\\__init__.py:942: MatplotlibDeprecationWarning: nbagg.transparent is deprecated and ignored. Use figure.facecolor instead.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b8c7910b6444179d8917885cad1a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_degree = 40\n",
    "degree_range = np.arange(1,max_degree)\n",
    "bias = np.zeros(max_degree - 1)\n",
    "variance = np.zeros(max_degree - 1)\n",
    "regs = []\n",
    "\n",
    "for poly_d in degree_range:\n",
    "    X_train = np.array([x**d for d in range(1,poly_d+1)]).T\n",
    "    X_cv = np.array([x_cv**d for d in range(1,poly_d+1)]).T\n",
    "    reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "    bias[poly_d-1] = np.linalg.norm(y_train - reg.predict(X_train))**2\n",
    "    variance[poly_d-1] = np.linalg.norm(y_cv - reg.predict(X_cv))**2\n",
    "    regs.append(reg)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(degree_range, bias, color='b', label='bias')\n",
    "ax.plot(degree_range, variance, color='r', label='variance')\n",
    "ax.set_xlabel('degree')\n",
    "ax.set_ylabel('value')\n",
    "ax.set_xticks(np.arange(max_degree))\n",
    "ax.tick_params(axis=\"x\", labelsize=6)\n",
    "ax.text(4,20, 'ellbow kink')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when we increase the degree, the bias and the variance both go down substantially for the low degrees and then remain almost unchanged. This phenomenon is called the ''ellbow kink'': Imagine an arm where the shoulder is located at degree 0 here and the hand stretching towards the degree axis. The graph looks like the ellbow and there is a clear kink. Determining the optimal degree via the ellbow kink means, that we chose the degree where the kink occurs, which is $d=3$ here. Notice that when the data is more noisy or in general less synthetic, the picture might not be so clear and some judgement call needs to be made.\n",
    "\n",
    "We also observe that when the degree of the polynomial gets very high, the variance starts to explode again. This is a clear sign of overfitting. In this case it is also a sign of also numercial problems, as degrees bigger than $20$ are not really feasible at double precision anymore. Even if the data set would exhibit non-linearities, which could justify chosing such a high degree in the polynomial regression, one might want to consider other techniques then. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the result for the optimal degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee2a0df7bcd4569b9137444d4da722d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poly_d = 3\n",
    "X_train = np.array([x**d for d in range(1,poly_d+1)]).T\n",
    "reg = regs[2]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_train, y_train, s=dot_size, label='train')\n",
    "ax.scatter(x_cv, y_cv, s=dot_size, alpha=0.5, label='cv')\n",
    "ax.plot(x_train, reg.predict(X_train), color='r', label='fit', lw=1)\n",
    "format_axis(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot confirms that the regression with the optimal degree determined by the ellbow kink in the bias/variance diagnostic fits the data set quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare regression coefficients with true coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda3\\envs\\heroku\\lib\\site-packages\\matplotlib\\__init__.py:942: MatplotlibDeprecationWarning: nbagg.transparent is deprecated and ignored. Use figure.facecolor instead.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6869787496fe47f8b60365cc7552b102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef_fitted = np.r_[reg.intercept_, reg.coef_]\n",
    "coef_true = np.array([5., 0., 6., 7.])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xrange = np.arange(coef_fitted.shape[0])\n",
    "bwidth=0.2\n",
    "ax.bar(xrange-bwidth/2, coef_fitted, width=bwidth, color='r', label='fitted')\n",
    "rects = ax.bar(xrange+bwidth/2, coef_true, width=bwidth, color='tab:blue', label='true')\n",
    "ax.set_xticks(xrange)\n",
    "ax.set_xlabel('degree')\n",
    "ax.set_ylabel('value')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the coefficients obtained by regression with the optimal degree are quite close to the ''true'' coefficients used to generate the synthetic data set. If the data set is not synthetic, the ''true'' coefficients are of course not known and then this diagnostic cannot be obtained. We've added it here as a sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Polynomial Regression & Regularization\n",
    "\n",
    "Tweaking the degree of a univariate regression polynomial can be tricky. For multivariate polynomial regression, the task is even harder. Consider in input data set $x=(x_1, \\ldots, x_N)$ with samples $x_i \\in \\mathbb{R}^k$ for $k$ large. Even when dropping all cross-terms from the regression polynomial, we'd still have to choose degrees $d=(d_1, \\ldots, d_k)$ for every dimension. For $k=2,3$ one can still do this manually, but if $k=100$ or $k=1000$, then even with systematic diagnostics as above, manually tweaking the degree for every single dimension becomes unfeasible. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solution to this problem is to only chose one maximal degree $d_{\\max}$ for every dimension and adapt the cost functional $J$ to automatically penalize using too many degrees in every dimension. This technique is called *regularization*.\n",
    "\n",
    "**Definition (regularized regression)**: Let $X \\in \\mathbb{R}^{N \\times m}$ be a design matrix and $y \\in \\mathbb{R}^N$ be a response for a linear regression. The functional \n",
    "\\begin{align*}\n",
    "    J_{\\alpha}: \\mathbb{R}^m \\to \\mathbb{R}, && \\theta \\mapsto \\frac{1}{2N} \\Big( \\|X \\theta - y \\|_2^2 + \\alpha \\|\\theta\\|_2^2 \\Big)\n",
    "\\end{align*}\n",
    "is called *regularized least squares with regularization parameter $\\alpha \\in \\mathbb{R}_{\\geq 0}$*. The corresponding solution $\\beta$ of\n",
    "\\begin{align*}\n",
    "    J_{\\alpha}(h_{\\beta}(X),y) = \\min_{\\theta \\in \\mathbb{R}^m}{J_{\\alpha}(h_{\\theta}(X),y)}\n",
    "\\end{align*}\n",
    "is called *Lasso regression* (or *Tikhonov regression*). One can also replace $\\|\\theta\\|_2^2 = \\sum_{j=1}^m{\\theta_j^2}$ with $\\|\\theta\\|_1 = \\sum_{j=1}^m{|\\theta_j|}$. The resulting solution is then called *Ridge regression*.\n",
    "\n",
    "**Remark**: We can think of the regularized functional $J_{\\alpha}$ as a functional that tries to simultaneously solve two minimization objectives: \n",
    "* The term $\\|X \\theta - y \\|_2$ is the same as for the unregularized regression, which corresponds to $\\alpha=0$.\n",
    "* The term $\\|\\theta\\|_2$ is obviously smallest for $\\theta=0$. That means that for $\\alpha>0$, the functional $J_{\\alpha}$ tries to make the squares as small as possible, but with coefficients also as small as possible. Thus, this term penalizes the use of coefficients, which do not substantially improve the optimization objective. The parameter $\\alpha$ controls the trade-off between smaller squares and smaller coefficients.\n",
    "\n",
    "The Lasso regression can be solved by techniques similar to the unregularized regression. For the Ridge regression, the term $\\|\\theta\\|_1$ makes this equation structurally so different that other optimization techniques have to be used. We do not discuss this here, but rather use the implementation provided by `scikit-learn`.\n",
    "\n",
    "Regularization simplifies the use of multivariate regression in cases where the samples $x_i \\in \\mathbb{R}^k$ for $k$ large. We chose the maximal degree $d_{\\max}$, e.g. $d_{\\max}=5$, and then the resulting design matrix has dimensions $X \\in \\mathbb{R}^{N \\times m}$ with $m := k \\cdot d_{\\max} +1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We apply the above regularization technique for a synthetic example, in which all dimensions are still small enough such that everything can be visualized in 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda3\\envs\\heroku\\lib\\site-packages\\matplotlib\\__init__.py:942: MatplotlibDeprecationWarning: nbagg.transparent is deprecated and ignored. Use figure.facecolor instead.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf38b6a1cf5403fa64b5c3af4d73a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_training_set(num_samples_dim):\n",
    "    x1 = np.linspace(-1, 1, num_samples_dim)\n",
    "    x2 = np.linspace(-1, 1, num_samples_dim)\n",
    "    xx1, xx2 = np.meshgrid(x1, x2)\n",
    "    X = np.array([x**d for x in [xx1.flatten(), xx2.flatten()] for d in range(1,poly_d+1)]).T\n",
    "    return xx1, xx2, X\n",
    "\n",
    "num_samples = 10\n",
    "poly_d = 2\n",
    "xx1_train, xx2_train, X_train = create_training_set(num_samples)\n",
    "y_train = np.array([x1val+2*x2val**2 for x1val, x2val in zip(xx1_train.flatten(), xx2_train.flatten())])\n",
    "y_train += np.random.normal(0, 0.1, y.shape[0])\n",
    "\n",
    "# same as training set, but finer grid for plotting\n",
    "grid_mult = 3\n",
    "xx1_grid, xx2_grid, X_grid = create_training_set(num_samples*grid_mult)\n",
    "\n",
    "reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(xx1_train.flatten(), xx2_train.flatten(), y_train, label='train')\n",
    "ax.scatter3D(xx1_grid.flatten(), xx2_grid.flatten(), reg.predict(X_grid), label='predict')\n",
    "ax.legend()\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "_ = ax.set_zlabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this data has a linear dependence in the $x_1$-direction and a quadratic dependence in the $x_2$-direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias/Variance diagnostic for multivariate regression\n",
    "We try out a large number of maximal degrees and compute the unregularized linear least squares regression as well as the Ridge and the Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda3\\envs\\heroku\\lib\\site-packages\\matplotlib\\__init__.py:942: MatplotlibDeprecationWarning: nbagg.transparent is deprecated and ignored. Use figure.facecolor instead.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee2aff59d9a47a1984d6ea0f3ea3d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create cross-validation set\n",
    "x1_cv = np.linspace(-1, 1, num_samples) + 1 / (2 * num_samples)\n",
    "x2_cv = np.linspace(-1, 1, num_samples) + 1 /(2 * num_samples)\n",
    "xx1_cv, xx2_cv = np.meshgrid(x1_cv, x2_cv)\n",
    "y_cv = np.array([x1val + 2 * x2val**2 for x1val, x2val in zip(xx1_cv.flatten(), xx2_cv.flatten())])\n",
    "y_cv += np.random.normal(0, 0.1, y_cv.shape[0])\n",
    "\n",
    "# execute bias/variance diagnostic for a range of maximal degrees\n",
    "max_degree = 15\n",
    "degree_range = np.arange(1, max_degree)\n",
    "reg_lin = []\n",
    "reg_ridge = []\n",
    "reg_lasso = []\n",
    "bias_lin = np.zeros(max_degree - 1)\n",
    "bias_ridge = np.zeros(max_degree - 1)\n",
    "bias_lasso = np.zeros(max_degree - 1)\n",
    "variance_lin = np.zeros(max_degree - 1)\n",
    "variance_ridge = np.zeros(max_degree - 1)\n",
    "variance_lasso = np.zeros(max_degree - 1)\n",
    "\n",
    "for poly_d in degree_range:\n",
    "    X_train = np.array([x**d for x in [xx1_train.flatten(), xx2_train.flatten()] for d in range(1, poly_d+1)]).T\n",
    "    X_cv = np.array([x**d for x in [xx1_cv.flatten(), xx2_cv.flatten()] for d in range(1, poly_d+1)]).T\n",
    "    reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "    ridge = Ridge(alpha=1, fit_intercept=True).fit(X_train, y_train)\n",
    "    lasso = Lasso(alpha=0.1, fit_intercept=True).fit(X_train, y_train)\n",
    "    reg_lin.append(reg)\n",
    "    reg_ridge.append(ridge)\n",
    "    reg_lasso.append(lasso)\n",
    "    bias_lin[poly_d-1] = np.linalg.norm(y_train - reg.predict(X_train))**2 \n",
    "    bias_ridge[poly_d-1] = np.linalg.norm(y_train - ridge.predict(X_train))**2\n",
    "    bias_lasso[poly_d-1] = np.linalg.norm(y_train - lasso.predict(X_train))**2 \n",
    "    variance_lin[poly_d-1] = np.linalg.norm(y_cv - reg.predict(X_cv))**2\n",
    "    variance_ridge[poly_d-1] = np.linalg.norm(y_cv - ridge.predict(X_cv))**2\n",
    "    variance_lasso[poly_d-1] = np.linalg.norm(y_cv - lasso.predict(X_cv))**2\n",
    "                                                \n",
    "# plot result\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(degree_range, bias_lin, color='b', label='bias_lin')\n",
    "ax.plot(degree_range, variance_lin, color='r', label='variance_lin')\n",
    "ax.plot(degree_range, bias_ridge, color='k', label='bias_ridge')\n",
    "ax.plot(degree_range, variance_ridge, color='g', label='variance_ridge')\n",
    "ax.plot(degree_range, bias_lasso, color='y', label='bias_lasso')\n",
    "ax.plot(degree_range, variance_lasso, color='c', label='variance_lasso')\n",
    "ax.legend()\n",
    "ax.set_xticks(degree_range)\n",
    "ax.set_xlabel('max_degree')\n",
    "ax.set_ylabel('value')\n",
    "ax.tick_params(axis=\"x\", labelsize=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "* In all all three cases - unregularized linear regression, Ridge regression and Lasso regression - there is a clear ellbow kink at $d_{\\max}=2$. \n",
    "* The Ridge regression (using $\\alpha=1$ here) has a much lower bias and variance than the Lasso regression despite using $\\alpha=0.1$, which makes this our favourite.\n",
    "* The unregularized linear regression has a variance, which starts to explode quite quickly at $d_{\\max}=8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare regression coefficients with true coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12bdb34b14844fb82aeb5d9c2008d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coefs_lin = np.r_[reg_lin[1].intercept_, reg_lin[1].coef_]\n",
    "coefs_ridge = np.r_[reg_ridge[1].intercept_, reg_ridge[1].coef_]  \n",
    "coefs_lasso = np.r_[reg_lasso[1].intercept_, reg_lasso[1].coef_]\n",
    "\n",
    "fig_, ax = plt.subplots()\n",
    "xrange = np.arange(coefs_lin.shape[0])\n",
    "bwidth = 0.5\n",
    "ax.bar(xrange-4*bwidth/4, coefs_lin, width=bwidth/4, color='r', label='linear')\n",
    "ax.bar(xrange-3*bwidth/4, coefs_ridge, width=bwidth/4, color='b', label='ridge')\n",
    "ax.bar(xrange-2*bwidth/4, coefs_lasso, width=bwidth/4, color='g', label='lasso')\n",
    "ax.bar(xrange-1*bwidth/4, np.array([0., 1., 0., 0., 2.]), width=bwidth/4, color='k', label='true')\n",
    "ax.set_xticks(xrange-bwidth/2)\n",
    "ax.set_xticklabels(['intercept', '$x_1$', '$x_2$', '$x_1^2$', '$x_2^2$'])\n",
    "ax.set_xlabel('coefficient')\n",
    "ax.set_ylabel('value')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data set is synthetic, we can compare the results of the three regressions with the true coefficients. \n",
    "* The unregularized linear regression yields coefficients, which are quite close to the true coefficients.\n",
    "* The Ridge regression is mostly close, but for the quadratic term already deviates a bit.\n",
    "* The Lasso regression yields coefficients, which are quite different.\n",
    "\n",
    "The take away from this is that different regularizations yield regression polynomials with quite different coefficients, even though their predictions are almost equally good. Thus, if the use case of the regression is to make predictions, this does not matter. However, if the aim of the regression is to obtain the coefficients and process these by downstream algorithms, the type of regularizations (and also the parameter $\\alpha$) makes quite a difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heroku",
   "language": "python",
   "name": "heroku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
